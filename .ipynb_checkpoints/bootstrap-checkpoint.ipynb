{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6840dfee",
   "metadata": {},
   "source": [
    "# Part 5.5: Benchmarking Regularization Techniques\n",
    "\n",
    "Quite a few hyperparameters have been introduced so far.  Tweaking each of these values can have an effect on the score obtained by your neural networks.  Some of the hyperparameters seen so far include:\n",
    "\n",
    "* Number of layers in the neural network\n",
    "* How many neurons in each layer\n",
    "* What activation functions to use on each layer\n",
    "* Dropout percent on each layer\n",
    "* L1 and L2 values on each layer\n",
    "\n",
    "To try out each of these hyperparameters you will need to run train neural networks with multiple settings for each hyperparameter.  However, you may have noticed that neural networks often produce somewhat different results when trained multiple times.  This is because the neural networks start with random weights.  Because of this it is necessary to fit and evaluate a neural network times to ensure that one set of hyperparameters are actually better than another.  Bootstrapping can be an effective means of benchmarking (comparing) two sets of hyperparameters.  \n",
    "\n",
    "Bootstrapping is similar to cross-validation.  Both go through a number of cycles/folds providing validation and training sets.  However, bootstrapping can have an unlimited number of cycles.  Bootstrapping chooses a new train and validation split each cycle, with replacement.  The fact that each cycle is chosen with replacement means that, unlike cross validation, there will often be repeated rows selected between cycles.  If you run the bootstrap for enough cycles, there will be duplicate cycles.\n",
    "\n",
    "In this part we will use bootstrapping for hyperparameter benchmarking.  We will train a neural network for a specified number of splits (denoted by the SPLITS constant).  For these examples we use 100.  We will compare the average score at the end of the 100.  By the end of the cycles the mean score will have converged somewhat.  This ending score will be a much better basis of comparison than a single cross-validation.  Additionally, the average number of epochs will be tracked to give an idea of a possible optimal value.  Because the early stopping validation set is also used to evaluate the the neural network as well, it might be slightly inflated.  This is because we are both stopping and evaluating on the same sample.  However, we are using the scores only as relative measures to determine the superiority of one set of hyperparameters to another, so this slight inflation should not present too much of a problem.\n",
    "\n",
    "Because we are benchmarking, we will display the amount of time taken for each cycle.  The following function can be used to nicely format a time span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32b56273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed/(60*60))\n",
    "    m = int((sec_elapsed%(60*60))/60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{} : {:>02}:{:>05.2f}\".format(h, m, s)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89bc4849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'24 : 00:00.00'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hms_string(86400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4e3ffe",
   "metadata": {},
   "source": [
    "### Bootstrapping for Regression\n",
    "\n",
    "Regression bootstrapping uses the **ShuffleSplit** object to perform the splits.  This is similar to **KFold** for cross validation, no balancing takes place.  To demonstrate this technique we will attempt to predict the age column for the jh-simple-dataset this data is loaded by the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b5cb67e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 14 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   id              2000 non-null   int64  \n",
      " 1   job             2000 non-null   object \n",
      " 2   area            2000 non-null   object \n",
      " 3   income          1941 non-null   float64\n",
      " 4   aspect          2000 non-null   float64\n",
      " 5   subscriptions   2000 non-null   int64  \n",
      " 6   dist_healthy    2000 non-null   float64\n",
      " 7   save_rate       2000 non-null   int64  \n",
      " 8   dist_unhealthy  2000 non-null   float64\n",
      " 9   age             2000 non-null   int64  \n",
      " 10  pop_dense       2000 non-null   float64\n",
      " 11  retail_dense    2000 non-null   float64\n",
      " 12  crime           2000 non-null   float64\n",
      " 13  product         2000 non-null   object \n",
      "dtypes: float64(7), int64(4), object(3)\n",
      "memory usage: 218.9+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/jh-simple-dataset.csv\",\n",
    "    na_values = ['NA','?'])\n",
    "df.info()\n",
    "\n",
    "df = pd.concat([df, pd.get_dummies(df['job'], prefix='job')], axis = 1)\n",
    "df.drop('job', axis = 1, inplace = True)\n",
    "\n",
    "df = pd.concat([df, pd.get_dummies(df['area'], prefix='area')], axis = 1)\n",
    "df.drop('area', axis = 1, inplace = True)\n",
    "\n",
    "df = pd.concat([df, pd.get_dummies(df['product'], prefix='product')], axis = 1)\n",
    "df.drop('product', axis = 1, inplace = True)\n",
    "\n",
    "#missing values for income\n",
    "df['income'] = df['income'].fillna(df['income'].median())\n",
    "\n",
    "df['income'] = zscore(df['income'])\n",
    "df['aspect'] = zscore(df['aspect'])\n",
    "df['save_rate'] = zscore(df['save_rate'])\n",
    "df['subscriptions'] = zscore(df['subscriptions'])\n",
    "\n",
    "#convert to numpy classifications\n",
    "x_columns = df.columns.drop('age').drop('id')\n",
    "x = df[x_columns].values\n",
    "y = df['age'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed374484",
   "metadata": {},
   "source": [
    "The following code performs the bootstrap.  The architecture of the neural network can be adjusted to compare many different configurations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5afae486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1 : score = 0.699821,+          mean Score = 0.699821, stdev = 0.000000,+          epochs = 100, mean epochs = 100, +          time = 0 : 00:06.99\n",
      "#2 : score = 1.030044,+          mean Score = 0.864933, stdev = 0.165112,+          epochs = 113, mean epochs = 106, +          time = 0 : 00:07.66\n",
      "#3 : score = 0.655314,+          mean Score = 0.795060, stdev = 0.167150,+          epochs = 103, mean epochs = 105, +          time = 0 : 00:08.08\n",
      "#4 : score = 0.555191,+          mean Score = 0.735093, stdev = 0.178164,+          epochs = 101, mean epochs = 104, +          time = 0 : 00:08.58\n",
      "#5 : score = 0.689171,+          mean Score = 0.725908, stdev = 0.160410,+          epochs = 140, mean epochs = 111, +          time = 0 : 00:10.02\n",
      "#6 : score = 1.039589,+          mean Score = 0.778188, stdev = 0.187373,+          epochs = 92, mean epochs = 108, +          time = 0 : 00:06.79\n",
      "#7 : score = 0.677191,+          mean Score = 0.763760, stdev = 0.177038,+          epochs = 112, mean epochs = 108, +          time = 0 : 00:08.89\n",
      "#8 : score = 0.549787,+          mean Score = 0.737013, stdev = 0.180089,+          epochs = 126, mean epochs = 110, +          time = 0 : 00:10.05\n",
      "#9 : score = 0.744434,+          mean Score = 0.737838, stdev = 0.169806,+          epochs = 158, mean epochs = 116, +          time = 0 : 00:15.74\n",
      "#10 : score = 0.659502,+          mean Score = 0.730004, stdev = 0.162797,+          epochs = 100, mean epochs = 114, +          time = 0 : 00:08.49\n",
      "#11 : score = 0.852972,+          mean Score = 0.741183, stdev = 0.159196,+          epochs = 130, mean epochs = 115, +          time = 0 : 00:10.22\n",
      "#12 : score = 0.662622,+          mean Score = 0.734636, stdev = 0.153957,+          epochs = 99, mean epochs = 114, +          time = 0 : 00:09.87\n",
      "#13 : score = 0.940487,+          mean Score = 0.750471, stdev = 0.157760,+          epochs = 92, mean epochs = 112, +          time = 0 : 00:07.65\n",
      "#14 : score = 0.707501,+          mean Score = 0.747402, stdev = 0.152424,+          epochs = 119, mean epochs = 113, +          time = 0 : 00:10.48\n",
      "#15 : score = 0.763951,+          mean Score = 0.748505, stdev = 0.147313,+          epochs = 109, mean epochs = 112, +          time = 0 : 00:10.44\n",
      "#16 : score = 0.776620,+          mean Score = 0.750262, stdev = 0.142798,+          epochs = 138, mean epochs = 114, +          time = 0 : 00:09.22\n",
      "#17 : score = 1.183805,+          mean Score = 0.775765, stdev = 0.172040,+          epochs = 90, mean epochs = 113, +          time = 0 : 00:06.53\n",
      "#18 : score = 0.700805,+          mean Score = 0.771600, stdev = 0.168072,+          epochs = 116, mean epochs = 113, +          time = 0 : 00:07.94\n",
      "#19 : score = 1.004207,+          mean Score = 0.783843, stdev = 0.171637,+          epochs = 111, mean epochs = 113, +          time = 0 : 00:08.31\n",
      "#20 : score = 1.079480,+          mean Score = 0.798625, stdev = 0.179271,+          epochs = 101, mean epochs = 112, +          time = 0 : 00:07.56\n",
      "#21 : score = 1.010400,+          mean Score = 0.808709, stdev = 0.180670,+          epochs = 93, mean epochs = 111, +          time = 0 : 00:08.09\n",
      "#22 : score = 0.616966,+          mean Score = 0.799994, stdev = 0.180978,+          epochs = 109, mean epochs = 111, +          time = 0 : 00:08.55\n",
      "#23 : score = 0.577359,+          mean Score = 0.790314, stdev = 0.182730,+          epochs = 151, mean epochs = 113, +          time = 0 : 00:10.76\n",
      "#24 : score = 0.764710,+          mean Score = 0.789247, stdev = 0.178956,+          epochs = 131, mean epochs = 113, +          time = 0 : 00:09.33\n",
      "#25 : score = 0.707799,+          mean Score = 0.785989, stdev = 0.176065,+          epochs = 114, mean epochs = 113, +          time = 0 : 00:08.52\n",
      "#26 : score = 0.699441,+          mean Score = 0.782660, stdev = 0.173447,+          epochs = 105, mean epochs = 113, +          time = 0 : 00:08.11\n",
      "#27 : score = 0.635314,+          mean Score = 0.777203, stdev = 0.172464,+          epochs = 97, mean epochs = 112, +          time = 0 : 00:06.81\n",
      "#28 : score = 0.586320,+          mean Score = 0.770386, stdev = 0.173021,+          epochs = 147, mean epochs = 114, +          time = 0 : 00:09.23\n",
      "#29 : score = 0.810411,+          mean Score = 0.771766, stdev = 0.170169,+          epochs = 115, mean epochs = 114, +          time = 0 : 00:08.01\n",
      "#30 : score = 1.003702,+          mean Score = 0.779497, stdev = 0.172411,+          epochs = 124, mean epochs = 114, +          time = 0 : 00:13.83\n",
      "#31 : score = 0.489760,+          mean Score = 0.770151, stdev = 0.177165,+          epochs = 120, mean epochs = 114, +          time = 0 : 00:12.45\n",
      "#32 : score = 0.775747,+          mean Score = 0.770326, stdev = 0.174377,+          epochs = 73, mean epochs = 113, +          time = 0 : 00:06.43\n",
      "#33 : score = 0.619399,+          mean Score = 0.765752, stdev = 0.173653,+          epochs = 110, mean epochs = 113, +          time = 0 : 00:09.18\n",
      "#34 : score = 0.685564,+          mean Score = 0.763394, stdev = 0.171616,+          epochs = 101, mean epochs = 112, +          time = 0 : 00:08.56\n",
      "#35 : score = 0.611302,+          mean Score = 0.759048, stdev = 0.171034,+          epochs = 119, mean epochs = 113, +          time = 0 : 00:11.27\n",
      "#36 : score = 0.614662,+          mean Score = 0.755037, stdev = 0.170303,+          epochs = 102, mean epochs = 112, +          time = 0 : 00:10.91\n",
      "#37 : score = 0.791564,+          mean Score = 0.756025, stdev = 0.168090,+          epochs = 101, mean epochs = 112, +          time = 0 : 00:12.17\n",
      "#38 : score = 1.281247,+          mean Score = 0.769846, stdev = 0.185954,+          epochs = 110, mean epochs = 112, +          time = 0 : 00:13.70\n",
      "#39 : score = 0.926192,+          mean Score = 0.773855, stdev = 0.185211,+          epochs = 118, mean epochs = 112, +          time = 0 : 00:14.59\n",
      "#40 : score = 0.762368,+          mean Score = 0.773568, stdev = 0.182890,+          epochs = 103, mean epochs = 112, +          time = 0 : 00:11.59\n",
      "#41 : score = 0.875500,+          mean Score = 0.776054, stdev = 0.181329,+          epochs = 94, mean epochs = 111, +          time = 0 : 00:08.20\n",
      "#42 : score = 0.791421,+          mean Score = 0.776420, stdev = 0.179172,+          epochs = 132, mean epochs = 112, +          time = 0 : 00:12.26\n",
      "#43 : score = 0.810699,+          mean Score = 0.777217, stdev = 0.177152,+          epochs = 140, mean epochs = 113, +          time = 0 : 00:14.63\n",
      "#44 : score = 0.775030,+          mean Score = 0.777167, stdev = 0.175128,+          epochs = 73, mean epochs = 112, +          time = 0 : 00:07.98\n",
      "#45 : score = 0.807087,+          mean Score = 0.777832, stdev = 0.173227,+          epochs = 101, mean epochs = 111, +          time = 0 : 00:09.46\n",
      "#46 : score = 0.656994,+          mean Score = 0.775205, stdev = 0.172238,+          epochs = 117, mean epochs = 111, +          time = 0 : 00:11.72\n",
      "#47 : score = 0.768465,+          mean Score = 0.775062, stdev = 0.170398,+          epochs = 120, mean epochs = 112, +          time = 0 : 00:11.60\n",
      "#48 : score = 0.488810,+          mean Score = 0.769098, stdev = 0.173500,+          epochs = 112, mean epochs = 112, +          time = 0 : 00:10.95\n",
      "#49 : score = 0.735013,+          mean Score = 0.768403, stdev = 0.171788,+          epochs = 121, mean epochs = 112, +          time = 0 : 00:11.90\n",
      "#50 : score = 0.853160,+          mean Score = 0.770098, stdev = 0.170475,+          epochs = 115, mean epochs = 112, +          time = 0 : 00:11.50\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import statistics\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "SPLITS = 50\n",
    "\n",
    "#Bootstrap\n",
    "boot = ShuffleSplit(n_splits = SPLITS, test_size = 0.1, random_state = 45)\n",
    "\n",
    "#Track Progress\n",
    "mean_benchmark = []\n",
    "epochs_needed = []\n",
    "num = 0\n",
    "\n",
    "#loop through samples\n",
    "\n",
    "for train, test in boot.split(x):\n",
    "    start_time = time.time()\n",
    "    num+=1\n",
    "    \n",
    "    #split train and test\n",
    "    x_train = x[train]\n",
    "    y_train = y[train]\n",
    "    x_test = x[test]\n",
    "    y_test = y[test]\n",
    "    \n",
    "    #build neural network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=x.shape[1], activation = 'relu'))\n",
    "    model.add(Dense(10, activation = 'relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss= 'mean_squared_error', optimizer = 'adam')\n",
    "    monitor = EarlyStopping(monitor = 'val_loss', min_delta = 1e-3,\n",
    "                           patience = 5, verbose = 0, mode = 'auto',\n",
    "                           restore_best_weights = True)\n",
    "    \n",
    "    # train the bootstrap samples\n",
    "    model.fit(x_train,y_train,validation_data=(x_test,y_test), callbacks=[monitor],verbose=0,epochs=1000)\n",
    "    epochs = monitor.stopped_epoch\n",
    "    epochs_needed.append(epochs)\n",
    "    \n",
    "    # predict on the boot up validation\n",
    "    pred = model.predict(x_test)\n",
    "    \n",
    "    #measure this bootstrap log loss\n",
    "    score = np.sqrt(metrics.mean_squared_error(pred, y_test))\n",
    "    mean_benchmark.append(score)\n",
    "    m1 = statistics.mean(mean_benchmark)\n",
    "    m2 = statistics.mean(epochs_needed)\n",
    "    mdev = statistics.pstdev(mean_benchmark)\n",
    "    \n",
    "    # record this iteration\n",
    "    time_look = time.time() - start_time\n",
    "    print(f\"#{num} : score = {score:.6f}, mean Score = {m1:.6f}, stdev = {mdev:.6f}, epochs = {epochs}, mean epochs = {int(m2)}, time = {hms_string(time_look)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4ccd4f",
   "metadata": {},
   "source": [
    "### Bootstrapping for Classification\n",
    "\n",
    "Regression bootstrapping uses the **StratifiedShuffleSplit** object to perform the splits.  This is similar to **StratifiedKFold** for cross validation, as the classes are balanced so that the sampling has no effect on proportions.  To demonstrate this technique we will attempt to predict the product column for the jh-simple-dataset this data is loaded by the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8e0bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Reading Datasets\n",
    "df = pd.read_csv(\"https://data.heatonresearch.com/data/t81-558/jh-simple-dataset.csv\",\n",
    "    na_values=['NA','?'])\n",
    "\n",
    "# Dummy value for job\n",
    "df = pd.concat([df, pd.get_dummies(df['job'], prefix =\"job\")], axis = 1)\n",
    "df.drop('job', axis = 1, inplace = True)\n",
    "\n",
    "# Dummy value for area\n",
    "df = pd.concat([df, pd.get_dummies(df['area'], prefix = \"area\")], axis = 1)\n",
    "df.drop('area', axis = 1, inplace = True)\n",
    "\n",
    "# Filling missing values for income\n",
    "df['income'] = df['income'].fillna(df['income'].median())\n",
    "\n",
    "# Standarize range\n",
    "df['income'] = zscore(df['income'])\n",
    "df['save_rate'] = ascore(df['save_rate'])\n",
    "df['subscriptions'] = zscore(df['subscriptions']) \n",
    "df['aspect'] = zscore(df['aspect'])\n",
    "df['age'] = zscore(df['age'])\n",
    "\n",
    "# Converting to numpy classifications\n",
    "x_columns = df.columns.drop('product').drop('id')\n",
    "x = df[x_columns].values\n",
    "dummies = pd.get_dummies(df['product'])\n",
    "products = dummies.columns\n",
    "y = dummies.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f3c53a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\KUSHAL~1\\AppData\\Local\\Temp/ipykernel_1136/2484645340.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mnum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'product'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mnum\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'split' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import statistics\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "SPLITS = 50\n",
    "\n",
    "boot = StratifiedShuffleSplit(n_splits = SPLITS, test_size = 0.1, random_state = 45)\n",
    "\n",
    "mean_benchmark = []\n",
    "epochs_needed = []\n",
    "num = 0\n",
    "\n",
    "for train, test in split(x, df['product']):\n",
    "    start_time = time.time()\n",
    "    num+=1\n",
    "    \n",
    "    x_train = x[train]\n",
    "    y_train = y[train]\n",
    "    x_test = x[test]\n",
    "    y_test = y[test]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_dim = x.shape[1], activation = 'relu'))\n",
    "    model.add(Dense(25, activation = 'relu'))\n",
    "    model.add(Dense(y.shape[1], activation = 'softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "    monitor = EarlyStopping(monitor = 'val_loss', patience = 25, min_delta = 1e-3,\n",
    "                            mode = 'auto', restore_best_weights = True)\n",
    "    model.fit(x_train, y_train, validation_data = (x_test, y_test),\n",
    "             callbacks = [monitor], verbose = 0, epochs = 1000)\n",
    "    epochs = monitor.epochs.stopped\n",
    "    epochs_needed.append(epochs)\n",
    "    \n",
    "    pred = model.predict(x_test)\n",
    "    \n",
    "    y_compare = np.argmax(y_test, axis = 1)\n",
    "    score = metrics.log_loss(y_compare, pred)\n",
    "    mean_benchmark.append(score)\n",
    "    m1 = statistics.mean(mean_benchmark)\n",
    "    m2 = statistics.mean(epochs_needed)\n",
    "    mdev = statistics.pstdev(mean_benchmark)\n",
    "    \n",
    "    time_took = time.time() - start_time\n",
    "    print(f\"#{num} : score = {score:.6f}, mean score = {m1:.6f}, stdev = {mdev:.6f}, epochs = {epochs}, mean_epochs = {int(m2)}, time = {hms_string(time_took)}\")\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b40f94d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
